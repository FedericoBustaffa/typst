= Structured Parallel Programming
<structured-parallel-programming>
Parallel programming is not easy and always have to find a way to
parallelize a program can be difficult. It is also true that in most
cases there are recurring patterns to achieve parallelization that can
be generalized and reused.

This is called #strong[structured parallel programming] and its aim is
to simplify the development of parallel applications by #emph[composing]
them from a small set of recurring patterns.

This approach have pros and cons and in every case is up to the
programmer to decide the best approach. In general we can say that the
structured approach is more readable, maintainable and more high-level.
In fact it provides patterns and implementations that a non expert in
parallel computation can use.

On the other side the unstructured approach can (or should) be more
efficient in terms of performance. The first motivation for an
unstructured approach is most of the time the need to build an ad-hoc
algorithm for the problem, trying to exploit all the possible
optimizations. This approach of course is difficult to maintain and it
is more error prone, because involve the usage of low level
synchronization mechanisms.

So in most cases the structured approach is better because there aren’t
so many ways of parallelize an algorithm and it’s also easier to think
about how to reduce a given problem to a combination of the known
"#strong[forms];".

== Terminology
<terminology>
Before go on let’s introduce some terminology, typical to structured
approach:

- #strong[Parallel patterns];: recurring high-level abstractions that
  describe common ways to organize parallel computation (pipeline, farm,
  scan, map ecc.).
- #strong[Algorithmic skeletons];: concrete implementations or library
  constructs that encapsulate specific parallel patterns into ready to
  use parallel components.
- #strong[Parallel paradigms];: broad conceptual frameworks or model for
  organizing and thinking about parallel computation. They provide an
  overall classification of parallelism approaches, incorporating both
  structured and unstructured methodologies.

So, for a more theoritical view we must concentrate with parallel
patterns, then we will analyze some skeleton by reviewing some library.

== Stream Parallelism
<stream-parallelism>
One possible type of parallelism is the so called #strong[stream
parallelism];, which is the way tasks flow from a #emph[computation
module] to another. In other works a #strong[stream of tasks] is an
ordered sequence of a computational or operational units (called tasks)
of the same type that are processed one after the other:

Each task in the stream typically represents a discrete piece of work,
such as a computation, data transformation, or process operation, which
contributes to the overall goal of the system or application.

Stream #strong[source] and #strong[sink] are respectively the first and
last stages of the computation workflow that produce and consume the
stream of tasks.

This can be related in some sense to the computation and communication
overlap that we try to obtain in distributed memory systems.

The two main parallel patterns that handle stream parallelism are
#strong[pipeline] and #strong[farm];, that typically aim to increase the
throughput.

== Data Parallelism
<data-parallelism>
The other possible kind of parallelism is #strong[data parallelism];,
which operates on a input collection of data and produces an ouput
collection of data.

This type of parallelism applies the same operation to many data
elements (or blocks) concurrently. The main data parallel patterns are:
#strong[map];, #strong[reduce];, #strong[stencil];, #strong[divide and
conquer] and #strong[scan] that have the main goal of reduce end to end
latency.

Typically data parallelism can involve the processing of so called
#strong[eso-streams];. Sometimes in fact there is no real stream, like a
network stream or a data stream from a sensor; in that case we can talk
about #strong[endo-stream];.

Typically an #emph[eso-stream] is a stream generated by a loop over a
data structure like an array. This introduce the field of #strong[data
parallelism];, in which, starting from a monolithic data structure, a
stream is generated by partitioning the structure, perform the parallel
computation and in the end recollect the results.

== References
<references>
- \[\[parallel\_computing\]\]
- \[\[pipeline\]\]
- \[\[farm\]\]
- \[\[map\]\]
- \[\[reduce\]\]
- \[\[stencil\]\]
- \[\[divide\_and\_conquer\]\]
