= Evolutionary Strategies
<evolutionary-strategies>
The aim of #strong[evolutionary strategies] is to optimize
#strong[continuous functions];, starting from a pool of individuals,
they are #strong[evaluated] and then #strong[mutated] and
#strong[recombined];.

== Mutation
<mutation>
These kind of algorithms heavily rely on #strong[mutation] in order to
explore the search space. In other words we want to modify every
individual by adding random noise, sampled from a gaussian distribution
$cal(N) (0 , 1)$, to its features.

At the beginning (generation $g = 0$) of the simulation a generic
individual $x^(g = 0)$ is sampled from the search space and represents a
candidate solution (a vector of features).

Another part of each individual is a set of #strong[strategy parameters]
$sigma^0$ that regulate how much that individual change (#emph[step
size];). So in total an individual is identified by the couple
$(x , sigma)$ where $x$ is the so called #strong[genotype];, while
$sigma$ is a set of #emph[endogenous] parameters that evolve with
individual but do not represent anything for the construction of the
solution.

For the mutation we can choose either

- #strong[Isotropic gaussian];: single distribution (and so single
  strategy parameter) for every dimension of the space
  $ x^(g + 1) = x^g + sigma dot.op cal(N) (0 , I) $ with $I$ the
  $n times n$ identity matrix, where $n$ is the number of features of
  $x^g$.
- #strong[Multiple gaussians];: one distributions (so one strategy
  parameters) for each dimension of the search space
  $ x^(g + 1) = x^g + [sigma_1 dot.op cal(N) (0 , 1) , dots.h , sigma_n cal(N) (0 , 1)] $
  or in a more compact form
  $ x^(g + 1) = x^g + D dot.op cal(N) (0 , 1) $ with $D$ diagonal matrix
  containing every $sigma_i$.

As said these endogenous parameters can evolve in the process, but
actually there are cases where keeping them constant results in a
failure of the optimization process, and so they should evolve alongside
the individual.

This lead to #strong[self-adaptive] strategy parameters, that for
example start with an high value, in order to move faster at the
beginning and promote exploration, and decrease over time, favoring
exploitation and let refine what we discover before.

== Recombination
<recombination>
The #strong[recombination] phase is instead defined by

- #strong[Mixing number] $rho$: the number of parents to generate one
  offspring. In case $rho = 1$ we are talking about #emph[cloning];.
- #strong[Offsprings number] $lambda$: the number of offsprings
  generated by a group of $rho$ parents. This can also bee seen as
  selecting $lambda$ groups of $rho$ parents that generate $1$ offspring
  each.

The actual recombination of features can be implemented in many ways and
always depends on the problem.

=== $(mu , lambda)$ - Strategy
<mu-lambda---strategy>
The $(mu , lambda)$ strategy, also known as #strong[comma strategy]
starts with a population $X^g$ and set of strategy parameters $sigma^g$
equal for all the features of a given individual (isotropic gaussian).
Then

+ Generate $lambda$ offsprings.
+ Select parents at random $P^g$.
+ Update the strategy values by recombination and mutation
  $ sigma_k^(g + 1) = upright("recombine") (sigma^g \| P^g) dot.op e^(epsilon.alt_k tilde.op cal(N) (0 , 1)) $
+ Update individuals by recombination and mutation with the updated
  strategy parameters
  $ x_k^(g + 1) = upright("recombine") (P^g) + sigma_k^(g + 1) dot.op cal(N) (0 , 1) $
+ Evaluate fitness.

The best $mu$ offsprings are kept out of all $lambda$ offsprings
($lambda > mu$) and the parents are always discarded.

Note that $lambda$ and $mu$ are so called #emph[exogenous] parameters
that are set once and never change during evolution (hyper-parameter).

Sometimes also the mixing number $rho$ is included in the formulation:
$(mu \/ rho , lambda)$.

In case of $mu = lambda$ there is no selection because all offsprings
are kept and all parents are discarded, and so itâ€™s like a random walk
in the search space.

This strategy is usually better in exploring the search space but can be
sub-optimal because of the the possible discard of some good individual.
Anyway for #emph[deceptive] problems it behaves fairly well.

=== $(mu + lambda)$ - Strategy
<mu-lambda---strategy-1>
The $(mu + lambda)$ strategy is basically the same as the #emph[comma]
strategy but now also parents compete for survival with offsprings and
as before the best $mu$ individuals are kept.

In this way if a parent is already the best solution that will ever be
discovered, it will survive until the end, implementing an
#strong[elitist policy] of survival. The #emph[comma] strategy instead
can discard a good solution and never find it again.

Sometimes also the mixing number $rho$ is included in the formulation:
$(mu \/ rho + lambda)$.

In case of $mu = lambda$ there is no particular issue because $lambda$
individuals are chosen out of a set of $mu + lambda$ individuals.

This strategy is usually better in exploiting the search space because
of its #emph[elitism] but can converge faster on local optima if the
problem is #emph[deceptive];.

== Covariance Matrix Adaptation
<covariance-matrix-adaptation>
One of the most popular evolution strategy is the #strong[covariance
matrix adaptation] (#strong[CMA];) which uses an #strong[adaptive]
covariance matrix to model the instensity and direction of the mutation.

The matrix adjust itself based on the local shape of the function
landscape, speeding up on plateaus and slowing down when there are
hills.

The population is sampled from a multivariate gaussian distribution

$ cal(N) (m^g , C^g) $

where $C^g$ is the covariance matrix at generation $g$, that is
initialized as the identity and where $m^g$ is the mean vector taken as
candidate solution, that is instead randomly initialized.

The basic structure of the algorithm is the following

+ Sample $lambda$ offsprings from the initial distribution
  - $x^(g + 1) = m^g + sigma dot.op cal(N) (0 , C^g)$
  - $x^g tilde.op cal(N) (m^g , sigma^2 dot.op C^g)$
+ Sort offsprings based on increasing fitness values.
+ Offsprings update mean vector values by truncated selection
  ($mu < lambda$) $ m^(g + 1) = sum_(i = 1)^mu w_i dot.op x_i^(g + 1) $
  where $mu$ is the number of offsprings selected.

The weights $w_1 gt.eq dots.h gt.eq w_mu > 0$ such that

$ sum_(i = 1)^mu w_i = 1 $

are introduced in order to update the mean vector with a weighted sum
that gives more importance to better individuals. In this way the new
center of the distribution is shifted towards the best offsprings (not
in the #emph[middle];).

So basically we have a multivariate gaussian distribution with the same
mean but with an adaptive variance in each direction of the space. Soon
or later the distribution will point in the direction of a local optima.

== References
<references>
- \[\[evolutionary\_optimization\]\]
