= Genetic Algorithms
<genetic-algorithms>
A branch of #emph[evolutionary optimization] that is quite popular is
relative to #strong[genetic algorithms];, that can be seen a general
purpose optimization protocols, initially designed to work on binary
representation of data, but that now can work basically on many more.

They are very useful for problems with several local optima,
high-dimensional and combinatorial problems, NP-complete problems and
multi-objective optimization because a population basically creates a
#emph[Pareto front] that can be explored.

The classical setup of a genetic algorithm is a population of
individuals representing candidate solutions. Each of them is
represented by a vector of features called #strong[chromosome];,
representing the #strong[genotype];.

The individual #strong[phenotype] can be generated by its genotype; for
example in the knapsack problem we have a binary representation of which
object is taken and which is not. The phenotype is list of taken object
with their total value and total weight.

The other part is given by the classical evolutionary algorithm loop, in
which, for each generation, we select some parents, recombine their
chromosomes, mutate the offsprings and, after evaluate them we keep some
based on fitness and some survival policy.

== Crossover
<crossover>
The operator that make possible to recombine genotypes is the
#strong[crossover];, that can be implemented in various ways and always
depends on the problem.

Usually before the crossover there is a #strong[selection] part where
parents are chosen proportionally to their fitness. After that their
chromosomes are combined.

The most used setup is to have many couples of $2$ parents that give
birth to $2$ offsprings with a #strong[$k$-point crossover];, with $k$
typically small ($1$ or $2$).

This kind of crossover split the chromosome in $k$ points for each
parents and then switching aligned pieces to create offsprings. For
example if we have parents

$ P_1 = 0100 quad P_2 = 1101 $

with a $1$-point crossover that splits the chromosome in two halves, we
obtain

$ P_1 = 01 lr(|00 quad P_2 = 11|) 01 $

and so the offsprings will be

$ O_1 = 0101 quad O_2 = 1100 $

Often the split point is randomly chosen at each generation or for each
couple of parents (anyway must be the same for each couple).

== Mutation
<mutation>
The #strong[mutation] is even more problem-based but usually mutates
with a low probability one (or a few) gene(s) in order to introduce some
variability.

An example of mutation for the previous example is to randomly choose a
gene and #emph[flip] it:

$ O_1 = 0101 arrow.r 0111 $

here we flipped the third bit.

== Encoding
<encoding>
The #strong[input encoding] is crucial in order to have an efficient
search; wrong encodings could also make it impossible to find a
solution.

There are two types of encoding:

- #strong[Direct];: each element of the genotype is associated with a
  feature in the phenotype. This means that for each phenotype feature
  is necessary to have a corresponding genotype representation. This is
  simple but does not scales well in more complex and structured
  problems.
- #strong[Indirect];: the genotype is like a set of rules to build the
  solution. This is more flexible but can be more challenging to design.

When we want to generate #strong[modular phenotypes];, indirect encoding
is generally better in every possible way, while with direct encoding we
can solve problems with a more fixed structure.

One of the main advantages of indirect encoding is the reduction of the
search space dimensionality, because one feature of the genotype can now
produce multiple features of the phenotype.

=== Compositional Patterns Producing Networks
<compositional-patterns-producing-networks>
A technique used in many fields is the so called #strong[compositional
patterns producing networks];, that is a popular way to design
#strong[developmental encodings];.

A developmental encoding is a function that takes phenotypicalâ€™s space
coordinates and produces phenotypical value as output.

Typically we have a network of functions that produces a pattern of some
kind, like an image. The process makes the network evolve in order to
produce images with some shape or pattern, or for example images that
approximates other images.

#figure(image("cppn.png"),
  caption: [
    CPPN|300
  ]
)

For example the genotype encodes the network topology, and the function
of each node. If for example we are trying to produce images we can now
just feed the network with points and get the result pixel value. With
direct encoding instead a genotype is composed by $m times n$ components
(the size of the picture).

So in this case the advantage of indirect encoding that we obtain a
function that can potentially produce images of some kind of arbitrary
dimensions (very flexible and modular). With direct encoding instead we
need to evolve again every feature (pixel) if we change $m$ or $n$.

#horizontalrule

For CPPN the most popular algorithm is #strong[NEAT] that uses a direct
encoding (all nodes and connections are in the genome) to produce
networks, that are the function for the indirect encoding.

The HyperNEAT algorithm is a cycle in which NEAT evolves a CPPN, that is
used to generate a neural network.

== References
<references>
- \[\[evolutionary\_optimization\]\]
